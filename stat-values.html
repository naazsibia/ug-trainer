<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Statistical Exercises: p‑Values & Effect Sizes</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" />
  <style>
    body { font-family: Arial, sans-serif; max-width: 800px; margin: auto; padding: 1rem; line-height: 1.6; background: #fefefe; color: #333; }
    nav a { margin-right: 1rem; color: #0366d6; text-decoration: none; }
    section { margin-top: 2rem; }
    pre { background: #2d2d2d; padding: 1rem; overflow-x: auto; white-space: pre; line-height: 1.5; margin-bottom: 1rem; border-radius: 4px; }
    code:not([class]) { background: #f6f8fa; padding: 0.2rem 0.4rem; border-radius: 3px; }
    .exercise { border: 1px dashed #ccc; padding: 1rem; margin: 1rem 0; border-radius: 4px; }
    button { display: inline-block; background-color: #28a745; color: #fff; padding: 0.5rem 1rem; border: none; border-radius: 4px; cursor: pointer; transition: background-color 0.3s ease; }
    button:hover { background-color: #218838; }
    h2 { border-bottom: 2px solid #e1e4e8; padding-bottom: 0.3rem; }
  </style>
</head>
<body>
  <h1>Exercises in Inferential Statistics</h1>
  <p>Welcome! This page introduces <strong>p‑values</strong> and <strong>effect sizes</strong>, why they matter, and common misconceptions. Try the exercises to build intuition before reading research papers.</p>
  <nav id="nav"></nav>
  <main id="content"></main>
  <footer>
    <p>Hosted on GitHub Pages at naazsibia.github.io/stats-exercises</p>
  </footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <script>
    const sections = [
      {
        id: 'pvalue-overview',
        title: 'What is a p‑Value?',
        html: `
<p>The <strong>p‑value</strong> quantifies the probability of observing data at least as extreme as your sample, assuming the <em>null hypothesis</em> is true. It bridges your observed data and a theoretical model:</p>
<ul>
  <li><code>p = 0.03</code> means there’s a 3% chance of data this extreme if the null is correct.</li>
  <li><strong>Statistical decision:</strong> Compare p to a pre-defined significance level (<em>α</em>, often 0.05). If <code>p &lt; α</code>, results are “statistically significant.”</li>
  <li><strong>Context matters:</strong> A p-value doesn’t measure practical importance—just how surprising data are under H₀.</li>
</ul>
<p>Key steps in hypothesis testing:</p>
<ol>
  <li>Define H₀ (no effect) and H₁ (alternative hypothesis).</li>
  <li>Choose α (e.g., 0.05) before collecting data.</li>
  <li>Compute test statistic (t, χ², F, etc.) and corresponding p-value.</li>
  <li>Decide: if p &lt; α, reject H₀; otherwise, fail to reject H₀.</li>
</ol>
<p><em>Tip:</em> Report exact p-values (e.g., p = 0.032) rather than just “p &lt; 0.05.” It shows transparency.</p>
`      },
      {
        id: 'pvalue-misconceptions',
        title: 'Common Misconceptions',
        html: `
<p><strong>What p‑values <em>do not</em> tell you:</strong></p>
<ul>
  <li>They are <em>not</em> the probability that the null hypothesis is true (<code>P(H₀|data)</code>).</li>
  <li>They don’t indicate effect size or scientific importance—only compatibility with H₀.</li>
  <li>They depend on sample size: large samples can yield small p even for trivial effects.</li>
</ul>
<p><strong>Why misinterpretation is risky:</strong></p>
<ul>
  <li><em>p‑hacking:</em> selectively reporting significant p-values inflates false-positive risk.</li>
  <li><em>Multiple comparisons:</em> performing many tests increases chance of at least one p &lt; α by luck.</li>
</ul>
<p><strong>Always pair p-values with:</strong>
<ul>
  <li>Effect sizes (e.g., Cohen’s d, Pearson’s r).</li>
  <li>Confidence intervals around estimates.</li>
  <li>Transparency about study design and sample size.</li>
</ul>`
      },
      {
        id: 'effect-size',
        title: 'Effect Sizes Explained',
        html: `
<p><strong>Effect size</strong> measures the magnitude of a phenomenon, providing scale-free insight. Common metrics:</p>
<ul>
  <li><code>d</code> (Cohen’s d): standardized mean difference between two groups.</li>
  <li><code>r</code> (correlation coefficient): strength and direction of linear association.</li>
  <li><code>η²</code> (eta squared): proportion of variance explained in ANOVA.</li>
</ul>
<p>Rule-of-thumb (Cohen, 1988):</p>
<ul>
  <li><code>d</code>: small ≈ 0.2, medium ≈ 0.5, large ≈ 0.8</li>
  <li><code>|r|</code>: small ≈ 0.1, medium ≈ 0.3, large ≈ 0.5</li>
</ul>
<p>Effect sizes help you judge practical significance and compare results across studies.</p>`
      },
      {
        id: 'exercise-1',
        title: 'Exercise 1: Interpreting p‑Values',
        html: `
<div class="exercise">
  <p><strong>Scenario:</strong> A drug trial reports <code>p = 0.04</code> comparing treatment vs placebo.</p>
  <ol>
    <li>Explain in your own words what this p‑value implies about the null hypothesis.</li>
    <li>Discuss why statistical significance alone isn’t enough without effect size.</li>
  </ol>
</div>`
      },
      {
        id: 'exercise-2',
        title: 'Exercise 2: Calculating Effect Size',
        html: `
<div class="exercise">
  <p>You have two groups:</p>
  <ul>
    <li>Group A scores: [85, 88, 90, 87]</li>
    <li>Group B scores: [80, 82, 78, 81]</li>
  </ul>
  <p>Compute Cohen’s d by hand:</p>
  <pre><code class="language-python">import numpy as np
A = np.array([85,88,90,87])
B = np.array([80,82,78,81])
mean_A, mean_B = A.mean(), B.mean()
pooled_sd = np.sqrt(((A.std(ddof=1)**2 + B.std(ddof=1)**2) / 2))
d = (mean_A - mean_B) / pooled_sd
print(f"Cohen's d = {d:.2f}")
</code></pre>
</div>`
      },
      {
        id: 'exercise-3',
        title: 'Exercise 3: p‑Value Pitfalls',
        html: `
<div class="exercise">
  <p><strong>Data dredging:</strong> You run 20 independent tests; one returns <code>p = 0.04</code>. How do multiple comparisons affect interpretation? What adjustment might you use (e.g., Bonferroni)?</p>
</div>`
      },
      {
        id: 'exercise-4',
        title: 'Exercise 4: Confidence Interval Interpretation',
        html: `
<div class="exercise">
  <p><strong>Scenario:</strong> A study reports a 95% confidence interval for a mean difference as [2.1, 5.7].</p>
  <ol>
    <li>Explain what this interval means in plain language.</li>
    <li>How does this complement the p-value?</li>
  </ol>
</div>`
      },
      {
        id: 'exercise-5',
        title: 'Exercise 5: Power and Sample Size',
        html: `
<div class="exercise">
  <p>You plan an experiment to detect a medium effect (d = 0.5) with α = 0.05 and power = 0.8.</p>
  <ol>
    <li>What sample size per group is required? (Hint: use a power table or software.)</li>
    <li>Discuss why low power can lead to non-significant p-values even with true effects.</li>
  </ol>
</div>`
      }
    ];
    const nav = document.getElementById('nav');
    const content = document.getElementById('content');
    sections.forEach(sec => {
      const link = document.createElement('a');
      link.href = `#${sec.id}`;
      link.textContent = sec.title;
      nav.appendChild(link);
      const section = document.createElement('section');
      section.id = sec.id;
      section.innerHTML = `<h2>${sec.title}</h2>${sec.html}`;
      content.appendChild(section);
    });
    Prism.highlightAll();
  </script>
</body>
</html>